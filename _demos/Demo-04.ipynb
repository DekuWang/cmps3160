{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pandas, Multiple Tables, and Tidy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, mount your google drive, change to the course folder, pull latest changes, and change to the lab folder.\n",
    "# Startup Magic to: (1) Mount Google Drive\n",
    "# (2) Change to Course Folder\n",
    "# (3) Pull latest Changes\n",
    "# (4) Move to the Demo Directory so that the data files are available\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My Drive/cmps3160\n",
    "!git pull\n",
    "%cd _demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Show a ludicrus number of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at some basic Pandas functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.DataFrame([0.3, 0.8, 0.1, 4.0, 1.2], \n",
    "                  index = ['a', 'c', 'd', 'e', 'f'], \n",
    "                  columns=['data'])\n",
    "\n",
    "display(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.DataFrame([0.9, 0.1, 4.8, 0.3], \n",
    "                  index = ['b', 'c', 'd', 'g'], \n",
    "                  columns=['data'])\n",
    "display(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-index can get used to do some funky things or make bigger frames.. [Doc Page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html)\n",
    "\n",
    "```Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use re-index to fill in some gaps if we want..\n",
    "import string\n",
    "\n",
    "display(s1.reindex(list(string.ascii_lowercase)[:10]))\n",
    "display(s1.reindex(list(string.ascii_lowercase)[:10], method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2[s2['data'] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1[ (s1['data'] < 1.0) & (s1['data'] > 3.0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1)\n",
    "display(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1+s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 - s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that + is like what we'll learn is an inner join soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at how we can use indicies to slice frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.loc['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.loc['a':'d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.loc['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1.iloc[0:3])\n",
    "display(s1.iloc[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2['data'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply functions across and down and with lambdas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.random.randn(4,3), columns=list('abc'),\n",
    "                 index=['Utah', 'Ohio', 'Texas','Oregon'])\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = lambda x: x.max() - x.min()\n",
    "\n",
    "frame.apply(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is axis=0 or per column, can also do per row!\n",
    "frame.apply(minmax, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Examples\n",
    "\n",
    "Some of the [Pandas Rank](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rank.html) commands aren't totally obvious..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'a':[0, 1, 0, 1],\n",
    "                      'b':[4.3, 7, -3, 2],\n",
    "                   'c':[-2, 5, 8, -2.5]})\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is average but you can do other things!\n",
    "\n",
    "display(frame.rank(ascending=False))\n",
    "\n",
    "display(frame.rank(ascending=False, method='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also sort along rows!\n",
    "\n",
    "frame.rank(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the slides to discuss basic GroupBy operations first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of using np.nan\n",
    "\n",
    "Note that we have to import numpy.nan but once we do we can use the groupby and other methods without having to worry about what to do with missing data.\n",
    "\n",
    "For the example below, what happens when we don't use NaNs in the data table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a data frame from a lists\n",
    "# Try replacing 'XX' with various values..\n",
    "\n",
    "df = pd.DataFrame({'age':     [12.2, 11.0, 15.6, '--'],\n",
    "                  'wgt_kg':   [42.3, 40.8, 65.3, 84.2],\n",
    "                  'hgt_cm':   [145.1, 143.8, 165.3, 185.8],\n",
    "                  'sex':      ['male', 'female', 'male', 'male'],\n",
    "                  'country': ['USA', 'AUS', 'EU', 'USA']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the type that has been imputed here...\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['age'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to make sure it's set as number!\n",
    "\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But first we have to make sure it's a NAN!!\n",
    "display(df[\"age\"].replace(\"--\", np.nan))\n",
    "\n",
    "df[\"age\"].replace(\"--\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Groupby Command\n",
    "\n",
    "Below we see what happens in Pandas when we use the groupby command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'age':     [12.2, 11.0, 15.6, 35.1],\n",
    "                  'wgt_kg':   [42.3, 40.8, 65.3, 84.2],\n",
    "                  'hgt_cm':   [145.1, 143.8, 165.3, 185.8],\n",
    "                  'sex':      ['male', 'female', 'male', 'male'],\n",
    "                  'country': ['USA', 'AUS', 'EU', 'USA']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['sex']).describe()\n",
    "\n",
    "# ?? Try describing it..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass operators to groupby to get better results.\n",
    "df.groupby(['sex']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also group by multiple columns\n",
    "df.groupby(['sex', 'country']).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also limit ourselves by just subselecting columns\n",
    "display(df.groupby([\"country\"]).count())\n",
    "display(df.groupby([\"country\"])[[\"sex\"]].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to a group that we made, use the [get_group() command](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html).\n",
    "\n",
    "We can also get access to the actual tuples and they're types that are generated by the `groupby` commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group('AUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or do it fancy?\n",
    "[display(grouped.get_group(i)) for i in grouped.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins in Pandas\n",
    "\n",
    "Let's see a few [joins in Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html) and how to execute them.\n",
    "\n",
    "**Note:** We can also use the [merge command in pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) to joins as well.  We'll explore this more fully in the lab.  The key idea is that merge let's us pick columns to do the merge/join while join will always join on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful here! We have to set the index!\n",
    "\n",
    "df1 = pd.DataFrame([(1, 'foo', 3), (2, 'bar', 2), (3, 'foo', 4), (4, 'foo', 3)],\n",
    "                  columns=['ID','A', 'B'])\n",
    "df1.set_index('ID', inplace=True)\n",
    "display(df1)\n",
    "\n",
    "df2 = pd.DataFrame([(1, 1.2), (2, 2.5), (3, 2.3), (5, 8.0)], \n",
    "                   columns=['ID','C'])\n",
    "df2.set_index('ID', inplace=True)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "df1.join(df2, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can do inner, outter, left, right.\n",
    "df1.join(df2, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't have the same column names we can tell Pandas that we want to join on a specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([(1, 'foo', 3), (2, 'bar', 2), (3, 'foo', 4), (4, 'foo', 3)],\n",
    "                  columns=['leftid','A', 'B'])\n",
    "display(df1)\n",
    "\n",
    "df2 = pd.DataFrame([(1, 1.2), (2, 2.5), (3, 2.3), (5, 8.0)], \n",
    "                   columns=['rightid','C'])\n",
    "df2.set_index('rightid', inplace=True)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "df1.join(df2, how='inner')\n",
    "# Is this what we wanted?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have set the index but we can also tell Pandas to do it for us.\n",
    "df1.join(df2, how='inner', on='leftid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Merge\n",
    "\n",
    "Or we can skip all this foolishness by using the [pandas merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) command..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can skip all this foolishness by using the merge command...\n",
    "df1 = pd.DataFrame([(1, 'foo', 3), (2, 'bar', 2), (3, 'foo', 4), (4, 'foo', 3)],\n",
    "                  columns=['ID','A', 'B'])\n",
    "display(df1)\n",
    "\n",
    "df2 = pd.DataFrame([(1, 1.2), (2, 2.5), (3, 2.3), (5, 8.0)], \n",
    "                   columns=['ID','C'])\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df1.merge(df2, left_on=\"ID\", right_on=\"ID\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complicated Indicies\n",
    "\n",
    "To set a **hierarchical index** one can refer to the [documetnation page](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a heiararichal index -- The bad way would be to build up tuples as an index.\n",
    "\n",
    "index = [('California', 2000), ('California', 2010),\n",
    "         ('New York', 2000), ('New York', 2010),\n",
    "         ('Texas', 2000), ('Texas', 2010)]\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "pop = pd.Series(populations, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can still get slices etc. in a strightforward way.\n",
    "pop[('California', 2010):('Texas', 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if we want to get all 2010 data we have to do something like...\n",
    "pop[[i for i in pop.index if i[1] == 2010]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can make the data frame as (note it is tidy!)\n",
    "df = pd.DataFrame([('California', 2000, 33871648),\n",
    "                   ('California', 2010, 37253956),\n",
    "                   ('New York', 2000, 18976457),\n",
    "                   ('New York', 2010, 19378102),\n",
    "                   ('Texas', 2000, 20851820),\n",
    "                   ('Texas', 2010, 25145561)],\n",
    "                  columns=['state', 'year', 'pop'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['state', 'year'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can do cool stuff slicing, but it gets compicated with tuples.\n",
    "df.loc[('California')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [df.xs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html) command can help with slicing multi-indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thought this can get a bit complicated... \n",
    "df.xs(2010, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melting and Tidy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/religon.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [melt command](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html) does...\n",
    "\n",
    "```\n",
    "DataFrame.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)\n",
    "\n",
    "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n",
    "\n",
    "This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’.\n",
    "```\n",
    "\n",
    "So here we want to take the above data frame and make it `grow down` by converting income into a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.melt(df,\n",
    "               [\"religion\"],\n",
    "               var_name=\"income\",\n",
    "               value_name=\"freq\")\n",
    "f_df = f_df.sort_values(by=[\"income\"])\n",
    "f_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use more simple slicing to see the things we need.\n",
    "\n",
    "f_df[(f_df['religion'] == 'Atheist')].sort_values(by='income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df[(f_df['income'] == '$50-75k')].sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More Complicated Example of Melting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df = pd.read_csv(\"./data/billboard.csv\")\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This thing is a mess!! What if want to say, graph the positions of a song for a given artist? Then how would we do it?\n",
    "\n",
    "Again, we are going to melt to make the table `grow down` by keeping the `id_vars` and then melting the rows based on week position to be values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep identifier variables\n",
    "id_vars = [\"year\",\n",
    "           \"artist.inverted\",\n",
    "           \"track\",\n",
    "           \"time\",\n",
    "           \"genre\",\n",
    "           \"date.entered\",\n",
    "           \"date.peaked\"]\n",
    "\n",
    "# Melt the rest into week and rank columns\n",
    "b_df = pd.melt(frame=b_df,\n",
    "             id_vars=id_vars,\n",
    "             var_name=\"week\",\n",
    "             value_name=\"rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(b_df.head(20))\n",
    "b_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the week thing, it's not good...\n",
    "b_df[\"week\"] = b_df['week'].str.extract('(\\d+)', expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df[\"week\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a subtle problem here, rank is an int but some are NAN's and we can't have [int-nan's...](https://pandas.pydata.org/pandas-docs/version/0.24/whatsnew/v0.24.0.html#optional-integer-na-support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not ints? Gotcha! ?!?!?\n",
    "b_df[\"rank\"] = b_df[\"rank\"].astype(float)\n",
    "\n",
    "display(b_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see if anything is missing...\n",
    "# Remember axis=1 goes row wise..so we are saying show us any row with missing values...\n",
    "b_df[b_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need these so... let's drop them.\n",
    "b_df = b_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the handy [to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) which will get us a date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"date\" columns\n",
    "#  date = (date entered chart) + (# of weeks) - (1 week) [fence post problem]\n",
    "b_df['date'] = pd.to_datetime(b_df['date.entered']) + pd.to_timedelta(b_df['week'], unit='w') - pd.DateOffset(weeks=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(b_df[100:110])\n",
    "b_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore now-redundant, messy columns -- same as dropping \n",
    "b_df = b_df[[\"year\",\n",
    "         \"artist.inverted\",\n",
    "         \"track\",\n",
    "         \"time\",\n",
    "         \"genre\",\n",
    "         \"week\",\n",
    "         \"rank\",\n",
    "         \"date\"]]\n",
    "\n",
    "b_df = b_df.sort_values(ascending=True, by=[\"year\",\"artist.inverted\",\"track\",\"week\",\"rank\"])\n",
    "\n",
    "# Keep tidy dataset for future usage\n",
    "billboard = b_df\n",
    "\n",
    "billboard.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see the artists we have...\n",
    "billboard['artist.inverted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier to visualize!!\n",
    "b_df[(b_df['artist.inverted'] == 'Jay-Z')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df[(b_df['artist.inverted'] == 'Jay-Z')]['rank'].plot.hist(bins=100, title=\"Jay-Z Hits by Week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot of JayZ at different songs/days?\n",
    "b_df[(b_df['artist.inverted'] == 'Jay-Z')].plot.scatter(y='rank', \n",
    "                                                        x='date',\n",
    "                                                        rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.pairplot(x_vars=[\"date\"], \n",
    "             y_vars=[\"rank\"], \n",
    "             data=b_df[(b_df['artist.inverted'] == 'TLC')], \n",
    "             hue=\"track\",\n",
    "             height=5)\n",
    "\n",
    "# # Hacky rotation?? -- https://github.com/mwaskom/seaborn/issues/867\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "\n",
    "# # Invert..\n",
    "for ax in g.axes.flat:\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can get the list of top ranked songs...\n",
    "b_df[(b_df['rank'] == 1.0)].groupby(['artist.inverted']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
