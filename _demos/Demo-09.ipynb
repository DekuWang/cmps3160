{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression and Missing Data\n",
    "\n",
    "In this notebook we will look at the effects missing data can have on conclusions you can draw from data.  We will also go over some practical implementations for linear regressions in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "# Load SQLITE\n",
    "import sqlite3\n",
    "# Load Stats\n",
    "from scipy import stats\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Show a ludicrus number of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this work we will be using data from: Generalized body composition prediction equation for men using simple measurement techniques\", K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.\n",
    "\n",
    "[Data availabe here.](http://staff.pubhealth.ku.dk/~tag/Teaching/share/data/Bodyfat.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Penrose Data\n",
    "df_penrose = pd.read_csv(\"./data/bodyfat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_penrose.head())\n",
    "# observations = ['Neck', 'Chest', 'Abdomen', 'Hip', 'Thigh', 'Knee', 'Ankle', 'Biceps', 'Forearm', 'Wrist']\n",
    "observations = ['Age', 'Neck', 'Forearm', 'Wrist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_penrose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some really basic scatter plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to look at some linear regressions of single variables to see what is going on!  So let's plot some regression lines.  Note that there are at least a few different ways -- [linregress](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html), [polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html), and [statsmodels](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "Here's a good article about it [Data science with Python: 8 ways to do linear regression and measure their speed](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a basic Linear Regression on a Single Variable.\n",
    "# Note that linregress p-value is whether or not the slope is 0, not if the correlation is significant.\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df_penrose[o],\n",
    "                                                                   df_penrose['bodyfat'])\n",
    "\n",
    "    # Pack these into a nice title\n",
    "    diag_str = \"p-value =\" + str(round(p_value, 7)) + \"\\n\" + \"r-value =\" + str(round(r_value, 7)) + \"\\nstd err. =\" + str(round(std_err, 7))\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', title=diag_str, ax=ax[i])\n",
    "    \n",
    "    # Make points and line\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = slope * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, color='red')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the polyfit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to fit a linear model with PolyFit.\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    # Fit our curve\n",
    "    x1, intercept = np.polyfit(df_penrose[o],df_penrose['bodyfat'], 1)\n",
    "    \n",
    "    # Plot regular points\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])\n",
    "    \n",
    "    # Plot curve\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = x1 * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, ls='-', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try fitting a degree 2 polynomial with polyfit.\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    \n",
    "    # Fit the polynomial.\n",
    "    x2, x1, intercept = np.polyfit(df_penrose[o],df_penrose['bodyfat'], 2)\n",
    "\n",
    "    # Plot our points.\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])\n",
    "    \n",
    "    # Plot the Regression Line..\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = x2 * pts**2 + x1 * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, ls='-', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try fitting a degree 5 polynomial with polyfit.\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    \n",
    "    # Fit the polynomial.\n",
    "    x5, x4, x3, x2, x1, intercept = np.polyfit(df_penrose[o],df_penrose['bodyfat'], 5)\n",
    "\n",
    "    # Plot our points.\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])\n",
    "    \n",
    "    # Plot the Regression Line..\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = x5 * pts**5 + x4 * pts**4 + x3 * pts**3 + x2 * pts**2 + x1 * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, ls='-', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complicated example with Statsmodels.\n",
    "\n",
    "Statsmodels (you'll likely need to install it) gives a much more R-like interface to linear modeling.  You can read [more about it here](https://www.statsmodels.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "df_ind = df_penrose[['Neck', 'Wrist']]\n",
    "df_target = df_penrose['bodyfat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ind\n",
    "y = df_target\n",
    "\n",
    "# Note the difference in argument order\n",
    "# Call: endog, then exog (dependent, indepenednt)\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "# Print out the statistics\n",
    "model.summary()\n",
    "#fig, ax = plt.subplots(figsize=(12,8))\n",
    "#fig = sm.graphics.plot_partregress(endog=\"bodyfat\", exog_i=['Abdomen', 'Neck'], exog_others='', data=df_penrose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [single regressor plot](https://tedboy.github.io/statsmodels_doc/generated/statsmodels.graphics.api.plot_partregress.html#statsmodels.graphics.api.plot_partregress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import plot_partregress\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_partregress(endog='bodyfat', exog_i='Neck', exog_others='', data=df_penrose, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have multiple elements in our regression then we need to use a different plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple regression plot\n",
    "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plot_partregress_grid(model, fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to work with regressions and their plots is using the [Seaborn Regression Package](https://seaborn.pydata.org/tutorial/regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do simple exploratory plots\n",
    "import seaborn as sns\n",
    "df_test = df_penrose.sample(frac=0.10, replace=False)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    sns.regplot(x=o, y='bodyfat', data=df_test, ax=ax[i])\n",
    "    #g.axes.set_xlim(df_test[o].min()*.95,df_test[o].max()*1.05)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice simulator to play with is [this one](https://ndirienzo.shinyapps.io/linear_regression_sim/) which is from [Prof. Nicholas DiRienzo](https://ischool.arizona.edu/people/nicholas-dirienzo) from ASU's School of Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We can use sklearn to do a quick logistic regression.  Remember that for logistic regression we are testing whether or not something is true, so we need to add a variable to our data.\n",
    "\n",
    "Someone is obese if their body fat is >32% so we'll add a dummy for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penrose['obese'] = df_penrose.apply(lambda x: 1 if x['bodyfat'] > 32 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use sklearn to build us a classifier.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setup our data for testing and training.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_penrose[observations],\n",
    "                                                    df_penrose['obese'],\n",
    "                                                    test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit that model!\n",
    "logisticRegr = LogisticRegression(max_iter=100000, class_weight='balanced') \n",
    "model = logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and plot!\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy Score is: {accuracy_score(y_test, y_pred)}\")\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                                 display_labels=logisticRegr.classes_,\n",
    "                                 cmap=plt.cm.Blues, normalize='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you do this again to test the various bits of the model?  Find out in Lab 9!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now back to Missing Data!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we start to remove parts of the data -- is the relationship still as strong?\n",
    "\n",
    "We can use the [pandas sample command](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) to remove some of the dataframe.\n",
    "\n",
    "Note that here we are just asking the question, if we took some of the data out randomly, do we still get the same result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's do a basic Linear Regression on a Single Variable.\n",
    "# Note that linregress p-value for the null-hyp that slope = 0.\n",
    "df_test = df_penrose.sample(frac=0.2, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i,o in enumerate(observations):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df_test[o],\n",
    "                                                                   df_test['bodyfat'])\n",
    "\n",
    "    # Pack these into a nice title\n",
    "    diag_str = \"p-value =\" + str(round(p_value, 7)) + \"\\n\" + \"r-value =\" + str(round(r_value, 7)) + \"\\nstd err. =\" + str(round(std_err, 7))\n",
    "    df_test.plot.scatter(x=o, y='bodyfat', title=diag_str, ax=ax[i])\n",
    "    \n",
    "    # Make points and line\n",
    "    pts = np.linspace(df_test[o].min(), df_test[o].max(), 500)\n",
    "    line = slope * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine if these correlations are significant under the missing data then we need to run bootstrap samples and see what happens.\n",
    "\n",
    "Nick -- modify this to drop part of the data then resample from the dropped part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {o:[] for o in observations}\n",
    "bootstrap_samples = 1000\n",
    "fraction = 0.20\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    for t in range(bootstrap_samples):\n",
    "        df_test = df_penrose.sample(frac=fraction, replace=False)\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(df_test[o],df_test['bodyfat'])\n",
    "        #r,p = stats.pearsonr(df_test[o], df_test['bodyfat'])\n",
    "        results[o].append(p_value)\n",
    "        \n",
    "rs = pd.DataFrame(results)\n",
    "ax = rs.boxplot()\n",
    "ax.set_ylim([-0.01,0.30])\n",
    "ax.set_title(f\"p-value of 1 variable regression over {bootstrap_samples} iterations\")\n",
    "ax.set_ylabel(\"p-value\")\n",
    "ax.set_xlabel(\"Body Part\")\n",
    "ax.axhline(y=0.05, lw=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above as we run more and more samples and plot the p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Text Analysis Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we go through a light example of processing a dataset for analyzing text!\n",
    "\n",
    "The data comes from this [website at Cornell](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and is from Bo Pang and Lillian Lee, A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, Proceedings of ACL 2004.\n",
    "\n",
    "This contains 1000 positive and 1000 negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are both in huge directories so let's make 2 data frames.\n",
    "\n",
    "import glob\n",
    "\n",
    "all_pos = glob.glob(\"./data/review_polarity/pos/*\")\n",
    "all_neg = glob.glob(\"./data/review_polarity/neg/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/review_polarity/pos/cv839_21467.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2cb83d1608da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's look at a review and see what's up..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/review_polarity/pos/cv839_21467.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/review_polarity/pos/cv839_21467.txt'"
     ]
    }
   ],
   "source": [
    "# Let's look at a review and see what's up..\n",
    "with open('./data/review_polarity/pos/cv839_21467.txt') as f:\n",
    "    x = f.readlines()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a little messy so let's clean out some of the stuff and join it into one documet.\n",
    "import re\n",
    "re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we're read to read and fix up each of the reviews.\n",
    "\n",
    "revs = []\n",
    "\n",
    "for fname in all_pos:\n",
    "    rec = {}\n",
    "    with open(fname) as f:\n",
    "        x = f.readlines()\n",
    "    rec['text'] = re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))\n",
    "    rec['sentiment'] = 'positive'\n",
    "    revs.append(rec)\n",
    "\n",
    "for fname in all_neg:\n",
    "    rec = {}\n",
    "    with open(fname) as f:\n",
    "        x = f.readlines()\n",
    "    rec['text'] = re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))\n",
    "    rec['sentiment'] = 'negative'\n",
    "    revs.append(rec)\n",
    "    \n",
    "df_reviews = pd.DataFrame(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the fun part...\n",
    "\n",
    "Now that we have some data to work with let's make some tf-idf vectors\n",
    "\n",
    "We're going to work with [tf-idf vectorizer from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "There are other options and a lot more you could do using sklearn! [See here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the whole thing...\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize and play with token sizes...\n",
    "vec = TfidfVectorizer(min_df = 0.01, \n",
    "                      max_df = 0.99, \n",
    "                      ngram_range=(2,2)) # play with min_df and max_df\n",
    "\n",
    "# transform this into a sparse vector!\n",
    "vec.fit(df_reviews['text'])\n",
    "tf_idf_sparse = vec.transform(df_reviews[\"text\"])\n",
    "tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing above is taking the reviews, and computing the tfidf scores for them if we cut off min_df and max_df, so we get letf with fewer words.  We can see the set of words along with the actual tfidf vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for a review we can see the ROW of it's encoding.\n",
    "\n",
    "print(tf_idf_sparse[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use this to classify the reviews!! but we need to test/train split again.\n",
    "\n",
    "# Split..\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_sparse, \n",
    "                                                    df_reviews['sentiment'], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(max_iter=100000, class_weight='balanced') \n",
    "model = logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always we should check our confusing matrix...\n",
    "# Check and confusion matrix...\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                                 display_labels=logisticRegr.classes_,\n",
    "                                 cmap=plt.cm.Blues, normalize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.coef_[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do cool stuff like make a dataframe with the words and see which\n",
    "# have the highest regression weights -- careful here!\n",
    "\n",
    "# Make a dataframe with the words, coefficients, and classes...\n",
    "recs = []\n",
    "for w,i in vec.vocabulary_.items():\n",
    "    recs.append([str(w)] + list(logisticRegr.coef_[:,i]))\n",
    "# If we only have one class then we only get weight..\n",
    "# df_weights = pd.DataFrame(tripples, columns=['word']+list(logisticRegr.classes_))\n",
    "df_weights = pd.DataFrame(recs, columns=['word', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.sort_values('weight', ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
