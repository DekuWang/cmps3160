{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 09 - Testing and Evaluating Models\n",
    "\n",
    "In this notebook we look at using text data to classify reviews, and some of the ways in which we can tell if our model is doing a good job at that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, mount your google drive, change to the course folder, pull latest changes, and change to the lab folder.\n",
    "# Startup Magic to: (1) Mount Google Drive\n",
    "# (2) Change to Course Folder\n",
    "# (3) Pull latest Changes\n",
    "# (4) Move to the Demo Directory so that the data files are available\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My Drive/cmps3160\n",
    "!git pull\n",
    "%cd _demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "# Load SQLITE\n",
    "import sqlite3\n",
    "# Load Stats\n",
    "from scipy import stats\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Show a ludicrus number of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we go through a light example of processing a dataset for analyzing text!\n",
    "\n",
    "The data comes from this [website at Cornell](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and is from Bo Pang and Lillian Lee, A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts, Proceedings of ACL 2004.\n",
    "\n",
    "This contains 1000 positive and 1000 negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to unzip the data.\n",
    "!mkdir -p ./data/review_polarity/\n",
    "!unzip ./data/review_polarity.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are both in huge directories so let's make 2 data frames.\n",
    "\n",
    "import glob\n",
    "\n",
    "all_pos = glob.glob(\"./data/review_polarity/pos/*\")\n",
    "all_neg = glob.glob(\"./data/review_polarity/neg/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a review and see what's up..\n",
    "with open('./data/review_polarity/pos/cv839_21467.txt') as f:\n",
    "    x = f.readlines()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a little messy so let's clean out some of the stuff and join it into one documet.\n",
    "import re\n",
    "re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we're read to read and fix up each of the reviews.\n",
    "\n",
    "revs = []\n",
    "\n",
    "for fname in all_pos:\n",
    "    rec = {}\n",
    "    with open(fname) as f:\n",
    "        x = f.readlines()\n",
    "    rec['text'] = re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))\n",
    "    rec['sentiment'] = 'positive'\n",
    "    revs.append(rec)\n",
    "\n",
    "for fname in all_neg:\n",
    "    rec = {}\n",
    "    with open(fname) as f:\n",
    "        x = f.readlines()\n",
    "    rec['text'] = re.sub(r'[.,\\'\\\"\\s\\s+]', \" \", \"\".join(x))\n",
    "    rec['sentiment'] = 'negative'\n",
    "    revs.append(rec)\n",
    "    \n",
    "df_reviews = pd.DataFrame(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the fun part...\n",
    "\n",
    "Now that we have some data to work with let's make some tf-idf vectors\n",
    "\n",
    "We're going to work with [tf-idf vectorizer from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "There are other options and a lot more you could do using sklearn! [See here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the whole thing...\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize and play with token sizes...\n",
    "vec = TfidfVectorizer(min_df = 0.01, \n",
    "                      max_df = 0.99, \n",
    "                      ngram_range=(2,2)) # play with min_df and max_df\n",
    "\n",
    "# transform this into a sparse vector!\n",
    "vec.fit(df_reviews['text'])\n",
    "tf_idf_sparse = vec.transform(df_reviews[\"text\"])\n",
    "tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing above is taking the reviews, and computing the tfidf scores for them if we cut off min_df and max_df, so we get letf with fewer words.  We can see the set of words along with the actual tfidf vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for a review we can see the ROW of it's encoding.\n",
    "\n",
    "print(tf_idf_sparse[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use this to classify the reviews!! but we need to test/train split again.\n",
    "\n",
    "# Split..\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_sparse, \n",
    "                                                    df_reviews['sentiment'], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(max_iter=100000, class_weight='balanced') \n",
    "model = logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always we should check our confusing matrix...\n",
    "# Check and confusion matrix...\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                                 display_labels=logisticRegr.classes_,\n",
    "                                 cmap=plt.cm.Blues, normalize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.coef_[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do cool stuff like make a dataframe with the words and see which\n",
    "# have the highest regression weights -- careful here!\n",
    "\n",
    "# Make a dataframe with the words, coefficients, and classes...\n",
    "recs = []\n",
    "for w,i in vec.vocabulary_.items():\n",
    "    recs.append([str(w)] + list(logisticRegr.coef_[:,i]))\n",
    "# If we only have one class then we only get weight..\n",
    "# df_weights = pd.DataFrame(tripples, columns=['word']+list(logisticRegr.classes_))\n",
    "df_weights = pd.DataFrame(recs, columns=['word', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.sort_values('weight', ascending=False)[:25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
